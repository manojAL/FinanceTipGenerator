# -*- coding: utf-8 -*-
"""FinanceTipGenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sSlrwfIvJzK7D_pDgCJJ3_a3_LK5slx6
"""

!pip install numpy pandas scikit-learn xgboost matplotlib seaborn joblib

# 2. Data Preparation and Feature Engineering - FINAL FIXED VERSION
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Set random seed for reproducibility
np.random.seed(42)

# =============================================
# Synthetic Data Generation - VERIFIED WORKING
# =============================================

def generate_financial_data(num_samples=10000):
    """Generate synthetic financial data for training"""
    # Create base columns first
    base_data = {
        'age': np.random.randint(18, 70, size=num_samples),
        'income': np.clip(np.random.lognormal(mean=10, sigma=0.4, size=num_samples).astype(int), 20000, 200000),
        'savings_rate': np.random.uniform(0.0, 0.3, size=num_samples),
        'credit_score': np.random.randint(300, 850, size=num_samples),
        'emergency_fund': np.random.choice([0, 1], size=num_samples, p=[0.6, 0.4]),
        'investment_status': np.random.choice(['none', 'basic', 'diversified'], size=num_samples, p=[0.5, 0.3, 0.2]),
        'region': np.random.choice(['urban', 'suburban', 'rural'], size=num_samples),
        'marital_status': np.random.choice(['single', 'married', 'divorced', 'widowed'], size=num_samples)
    }

    df = pd.DataFrame(base_data)

    # Now create expense columns that depend on income
    expense_factors = {
        'housing_cost': np.random.uniform(0.2, 0.4, size=num_samples),
        'food_spending': np.random.uniform(0.1, 0.25, size=num_samples),
        'transportation': np.random.uniform(0.05, 0.15, size=num_samples),
        'entertainment': np.random.uniform(0.03, 0.1, size=num_samples),
        'debt_payments': np.random.uniform(0.0, 0.3, size=num_samples)
    }

    for expense, factor in expense_factors.items():
        df[expense] = (df['income'] * factor).round(2)

    # Calculate derived metrics
    df['total_expenses'] = df[['housing_cost', 'food_spending', 'transportation',
                             'entertainment', 'debt_payments']].sum(axis=1).round(2)
    df['disposable_income'] = (df['income'] - df['total_expenses']).round(2)

    # Calculate financial health score (0-3)
    df['financial_health'] = (
        (df['savings_rate'] > 0.15).astype(int) +
        (df['debt_payments']/df['income'] < 0.2).astype(int) +
        (df['credit_score'] > 700).astype(int)
    )

    # Categorize financial health
    df['financial_health_category'] = pd.cut(
        df['financial_health'],
        bins=[-1, 1, 2, 3],
        labels=['poor', 'average', 'good']
    )

    return df

# Generate and inspect data
financial_data = generate_financial_data()
print("Data generated successfully. First 3 rows:")
print(financial_data.head(3))

# =============================================
# Data Exploration
# =============================================

print("\nData Summary:")
print(f"Rows: {financial_data.shape[0]}, Columns: {financial_data.shape[1]}")
print("\nMissing values per column:")
print(financial_data.isna().sum())

# =============================================
# Feature Engineering Pipeline - FIXED VERSION
# =============================================

class FinancialRatioTransformer(BaseEstimator, TransformerMixin):
    """Adds important financial ratios to the data"""
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Ensure we're working with a DataFrame
        X = pd.DataFrame(X, columns=[
            'age', 'income', 'housing_cost', 'food_spending',
            'transportation', 'entertainment', 'debt_payments',
            'credit_score', 'total_expenses', 'disposable_income'
        ])

        # Calculate ratios
        X['debt_to_income'] = X['debt_payments'] / X['income']
        X['expense_ratio'] = X['total_expenses'] / X['income']
        X['housing_ratio'] = X['housing_cost'] / X['income']

        # We'll add savings_rate later in the pipeline
        return X

def create_preprocessor():
    # Numerical features to transform
    num_features = [
        'age', 'income', 'housing_cost', 'food_spending',
        'transportation', 'entertainment', 'debt_payments',
        'credit_score', 'total_expenses', 'disposable_income'
    ]

    # Categorical features to encode
    cat_features = ['region', 'marital_status', 'investment_status']

    # Special features that need different processing
    ratio_features = ['savings_rate', 'emergency_fund']

    # Numerical pipeline for core features
    num_pipeline = Pipeline([
        ('ratios', FinancialRatioTransformer()),
        ('scaler', StandardScaler())
    ])

    # Pipeline for ratio features (just scale them)
    ratio_pipeline = Pipeline([
        ('scaler', StandardScaler())
    ])

    # Categorical pipeline
    cat_pipeline = Pipeline([
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combined preprocessor
    preprocessor = ColumnTransformer([
        ('num', num_pipeline, num_features),
        ('ratio', ratio_pipeline, ratio_features),
        ('cat', cat_pipeline, cat_features)
    ])

    return preprocessor

# =============================================
# Data Splitting - WITH ALL REQUIRED COLUMNS
# =============================================

# Prepare features and target
X = financial_data.drop(['financial_health_category', 'financial_health'], axis=1)
y = financial_data['financial_health_category']

# Verify all required columns are present
required_columns = [
    'age', 'income', 'housing_cost', 'food_spending', 'transportation',
    'entertainment', 'debt_payments', 'credit_score', 'total_expenses',
    'disposable_income', 'savings_rate', 'emergency_fund', 'region',
    'marital_status', 'investment_status'
]
print("\nChecking for missing columns:")
missing = set(required_columns) - set(X.columns)
if missing:
    print(f"Warning: Missing columns {missing}")
else:
    print("All required columns present")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("\nData split results:")
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# =============================================
# Test Preprocessing - FINAL WORKING VERSION
# =============================================

preprocessor = create_preprocessor()

# Fit and transform training data
try:
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    print("\nPreprocessing successful!")
    print(f"Training shape after preprocessing: {X_train_preprocessed.shape}")
    print(f"Test shape after preprocessing: {X_test_preprocessed.shape}")

    # Save preprocessor
    joblib.dump(preprocessor, 'preprocessor.joblib')
    print("\nPreprocessor saved to 'preprocessor.joblib'")

except Exception as e:
    print(f"\nError during preprocessing: {str(e)}")
    print("\nDebugging info:")
    print("X_train columns:", X_train.columns.tolist())
    print("Expected columns in num_features:", [
        'age', 'income', 'housing_cost', 'food_spending',
        'transportation', 'entertainment', 'debt_payments',
        'credit_score', 'total_expenses', 'disposable_income'
    ])
    print("Expected ratio features:", ['savings_rate', 'emergency_fund'])
    print("Expected categorical features:", ['region', 'marital_status', 'investment_status'])

# 3. Model Building and Training - FINAL CORRECTED VERSION
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import pandas as pd

# =============================================
# Load Preprocessed Data and Encode Target
# =============================================

# Load the preprocessor
preprocessor = joblib.load('preprocessor.joblib')

# Encode the target variable
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Save the label encoder for later use
joblib.dump(label_encoder, 'label_encoder.joblib')

# =============================================
# Model Selection - UPDATED
# =============================================

def get_models():
    """Return a dictionary of models to evaluate"""
    models = {
        'Random Forest': RandomForestClassifier(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss')
    }
    return models

# =============================================
# Baseline Model Evaluation - FULLY CORRECTED
# =============================================

def evaluate_models(X_train, y_train_encoded, X_test, y_test_encoded):
    """Evaluate multiple models and return their performance"""
    models = get_models()
    results = {}

    for name, model in models.items():
        # Create full pipeline
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', model)
        ])

        # Train the model
        print(f"\nTraining {name}...")
        pipeline.fit(X_train, y_train_encoded)

        # Evaluate using encoded labels throughout
        y_pred_encoded = pipeline.predict(X_test)
        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)

        # Get classification report with original labels
        y_pred = label_encoder.inverse_transform(y_pred_encoded)
        y_test = label_encoder.inverse_transform(y_test_encoded)
        report = classification_report(y_test, y_pred)

        # Store results
        results[name] = {
            'model': pipeline,
            'accuracy': accuracy,
            'report': report
        }

        print(f"{name} Accuracy: {accuracy:.4f}")

    return results

# Evaluate baseline models using encoded labels throughout
print("\nEvaluating baseline models...")
model_results = evaluate_models(X_train, y_train_encoded, X_test, y_test_encoded)

# =============================================
# Hyperparameter Tuning - UPDATED
# =============================================

def tune_xgboost(X_train, y_train_encoded):
    """Perform hyperparameter tuning for XGBoost"""
    # Create pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', XGBClassifier(random_state=42, eval_metric='mlogloss'))
    ])

    # Parameter grid
    param_grid = {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [3, 5, 7],
        'classifier__learning_rate': [0.01, 0.1, 0.2],
        'classifier__subsample': [0.8, 1.0],
        'classifier__colsample_bytree': [0.8, 1.0]
    }

    # Grid search with cross-validation
    grid_search = GridSearchCV(
        pipeline,
        param_grid,
        cv=3,
        scoring='accuracy',
        verbose=1,
        n_jobs=-1
    )

    print("\nStarting hyperparameter tuning for XGBoost...")
    grid_search.fit(X_train, y_train_encoded)

    print("\nBest parameters found:")
    print(grid_search.best_params_)

    return grid_search.best_estimator_

# Tune the best performing model (XGBoost)
best_model = tune_xgboost(X_train, y_train_encoded)

# =============================================
# Final Model Evaluation - UPDATED
# =============================================

def evaluate_final_model(model, X_test, y_test_encoded):
    """Evaluate the final tuned model"""
    print("\nEvaluating final model...")

    # Make predictions
    y_pred_encoded = model.predict(X_test)
    y_pred = label_encoder.inverse_transform(y_pred_encoded)
    y_test = label_encoder.inverse_transform(y_test_encoded)

    # Calculate metrics
    accuracy = accuracy_score(y_test_encoded, y_pred_encoded)
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    print(f"Final Model Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(report)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.savefig('confusion_matrix.png')
    plt.show()

    return {
        'accuracy': accuracy,
        'report': report,
        'confusion_matrix': cm
    }

# Evaluate the tuned model
final_results = evaluate_final_model(best_model, X_test, y_test_encoded)

# =============================================
# Feature Importance Analysis
# =============================================

def plot_feature_importance(model):
    """Plot feature importance for the trained model"""
    try:
        # Get feature names from the preprocessor
        # Numerical features
        num_features = [
            'age', 'income', 'housing_cost', 'food_spending',
            'transportation', 'entertainment', 'debt_payments',
            'credit_score', 'total_expenses', 'disposable_income',
            'debt_to_income', 'expense_ratio', 'housing_ratio'
        ]

        # Get categorical feature names
        cat_transformer = preprocessor.named_transformers_['cat']
        if hasattr(cat_transformer, 'named_steps'):
            cat_transformer = cat_transformer.named_steps['onehot']
        cat_features = cat_transformer.get_feature_names_out([
            'region', 'marital_status', 'investment_status'
        ])

        # Combine all feature names
        all_features = num_features + list(cat_features)

        # Get feature importances
        importances = model.named_steps['classifier'].feature_importances_

        # Create DataFrame and sort
        feature_importance = pd.DataFrame({
            'feature': all_features,
            'importance': importances
        }).sort_values('importance', ascending=False)

        # Plot top 20 features
        plt.figure(figsize=(12, 8))
        sns.barplot(
            x='importance',
            y='feature',
            data=feature_importance.head(20),
            palette='viridis'
        )
        plt.title('Top 20 Feature Importances')
        plt.tight_layout()
        plt.savefig('feature_importances.png')
        plt.show()

    except Exception as e:
        print(f"\nCould not plot feature importances: {str(e)}")

# Plot feature importance
plot_feature_importance(best_model)

# =============================================
# Save Final Model
# =============================================

joblib.dump(best_model, 'financial_advisor_final_model.joblib')
print("\nFinal model saved to 'financial_advisor_final_model.joblib'")

# =============================================
# Sample Predictions - UPDATED
# =============================================

def make_sample_predictions(model, samples):
    """Make predictions on sample data"""
    print("\nMaking sample predictions...")
    for i, sample in enumerate(samples, 1):
        # Convert sample to DataFrame
        sample_df = pd.DataFrame([sample])

        # Make prediction
        pred_encoded = model.predict(sample_df)
        pred = label_encoder.inverse_transform(pred_encoded)
        proba = model.predict_proba(sample_df)

        print(f"\nSample {i}:")
        print("Input Features:")
        print(sample_df.iloc[0][['age', 'income', 'savings_rate', 'debt_payments']])
        print(f"\nPredicted Financial Health: {pred[0]}")
        print("Prediction Probabilities:")
        for class_idx, class_name in enumerate(label_encoder.classes_):
            print(f"{class_name}: {proba[0][class_idx]:.2%}")

# Sample data for prediction (same as before)
sample_users = [
    {
        'age': 32,
        'income': 65000,
        'housing_cost': 18000,
        'food_spending': 10000,
        'transportation': 5000,
        'entertainment': 4000,
        'debt_payments': 15000,
        'savings_rate': 0.05,
        'credit_score': 680,
        'emergency_fund': 0,
        'investment_status': 'none',
        'region': 'urban',
        'marital_status': 'single'
    },
    {
        'age': 45,
        'income': 120000,
        'housing_cost': 30000,
        'food_spending': 15000,
        'transportation': 8000,
        'entertainment': 6000,
        'debt_payments': 12000,
        'savings_rate': 0.18,
        'credit_score': 780,
        'emergency_fund': 1,
        'investment_status': 'diversified',
        'region': 'suburban',
        'marital_status': 'married'
    }
]

# Calculate derived fields for samples
for sample in sample_users:
    sample['total_expenses'] = sum([
        sample['housing_cost'], sample['food_spending'],
        sample['transportation'], sample['entertainment'],
        sample['debt_payments']
    ])
    sample['disposable_income'] = sample['income'] - sample['total_expenses']

# Make predictions
make_sample_predictions(best_model, sample_users)

import pandas as pd
import joblib
import warnings
warnings.filterwarnings("ignore")  # Suppress sklearn warnings

# Load the trained model and preprocessor
model = joblib.load('financial_advisor_final_model.joblib')
preprocessor = joblib.load('preprocessor.joblib')
label_encoder = joblib.load('label_encoder.joblib')

def generate_financial_advice(predicted_class, user_data):
    """Generate personalized financial advice based on predicted class and user data."""

    advice = []

    # Savings Advice
    savings_rate = user_data['savings_rate']
    if predicted_class == 'poor':
        advice.append("🚨 **Savings**: You're saving less than 10% of your income. Aim for at least 15%.")
    elif predicted_class == 'average':
        advice.append("💰 **Savings**: You're doing okay, but try increasing savings to 20%.")
    else:
        advice.append("✅ **Savings**: Great job! Consider investing excess savings.")

    # Debt Advice
    debt_ratio = user_data['debt_payments'] / user_data['income']
    if debt_ratio > 0.35:
        advice.append("⚠️ **Debt**: Your debt is too high! Focus on paying off high-interest loans first.")
    elif debt_ratio > 0.2:
        advice.append("📉 **Debt**: Manageable but could be better. Consider refinancing.")
    else:
        advice.append("👍 **Debt**: You're handling debt well. Keep it up!")

    # Budgeting Advice
    expense_ratio = user_data['total_expenses'] / user_data['income']
    if expense_ratio > 0.9:
        advice.append("💸 **Budget**: Your spending is too high! Track expenses and cut unnecessary costs.")
    elif expense_ratio > 0.7:
        advice.append("📊 **Budget**: Review your spending habits. Try the 50/30/20 rule.")
    else:
        advice.append("📈 **Budget**: Well managed! Optimize further with automated tools.")

    # Emergency Fund Advice
    if user_data['emergency_fund'] == 0:
        advice.append("🆘 **Emergency Fund**: Build a 3-6 month safety net ASAP!")
    else:
        advice.append("🛡️ **Emergency Fund**: Good! Keep it in a high-yield savings account.")

    # Investment Advice
    investment_status = user_data['investment_status']
    if investment_status == 'none':
        advice.append("📉 **Investing**: Start with low-cost index funds (e.g., S&P 500 ETF).")
    elif investment_status == 'basic':
        advice.append("📊 **Investing**: Diversify into bonds and real estate.")
    else:
        advice.append("🚀 **Investing**: Explore tax-advantaged accounts (IRA, 401k).")

    return advice

def predict_financial_health(user_data):
    """Predict financial health and generate advice."""

    # Convert user data to DataFrame
    user_df = pd.DataFrame([user_data])

    # Calculate derived fields
    user_df['total_expenses'] = (
        user_df['housing_cost'] +
        user_df['food_spending'] +
        user_df['transportation'] +
        user_df['entertainment'] +
        user_df['debt_payments']
    )
    user_df['disposable_income'] = user_df['income'] - user_df['total_expenses']

    # Predict
    pred_encoded = model.predict(user_df)
    predicted_class = label_encoder.inverse_transform(pred_encoded)[0]

    # Generate advice
    advice = generate_financial_advice(predicted_class, user_data)

    return predicted_class, advice

def get_user_input():
    """Collect user financial data interactively."""
    print("\n💼 **Personal Finance Advisor** 💼")
    print("Please enter your financial details:\n")

    data = {
        'age': int(input("Age: ")),
        'income': float(input("Annual Income ($): ")),
        'housing_cost': float(input("Annual Housing Cost ($): ")),
        'food_spending': float(input("Annual Food Spending ($): ")),
        'transportation': float(input("Annual Transportation Costs ($): ")),
        'entertainment': float(input("Annual Entertainment Spending ($): ")),
        'debt_payments': float(input("Annual Debt Payments ($): ")),
        'savings_rate': float(input("Savings Rate (0-1, e.g., 0.1 for 10%): ")),
        'credit_score': int(input("Credit Score (300-850): ")),
        'emergency_fund': int(input("Emergency Fund? (0=No, 1=Yes): ")),
        'investment_status': input("Investment Status (none/basic/diversified): ").lower(),
        'region': input("Region (urban/suburban/rural): ").lower(),
        'marital_status': input("Marital Status (single/married/divorced/widowed): ").lower()
    }

    return data

